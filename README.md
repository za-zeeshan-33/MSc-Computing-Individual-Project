This project is on attributed question answering, and includes utilities for dataset preparation, prompt management, answer generation, and evaluation with AutoAIS.

The core idea: given a question and a set of passages, generate answers **with inline citations** (e.g. `[1][2]`), then evaluate citation quality and answer quality.

---

## 1. Environment setup

All examples below assume you are using a virtual environment.

Environment variables (especially `HF_TOKEN`) are loaded from `.env`:

## 2. Main components

- **`attribution_pipeline_improved.py`**  
  Core pipeline that generates cited answers given a dataset JSON and config.

- **`simplified_prompt_manager.py`**  
  Defines `SimplifiedPromptManager` and `MethodType` to build prompts for different datasets and methods (post-retrieval, post-generation, etc.).

- **`evaluation_autoais.py`**
  Core evaluation logic using a AutoAIS model.
---

## 3. Running the core pipeline directly

`attribution_pipeline_improved.py` defines a `main()` function with several CLI flags:

```bash
python attribution_pipeline_improved.py \
  --config config.json \
  --dataset natural_questions \
  --input my_datasets/dpr_nq/natural_questions_examples_4_to_103.json \
  --output outputs/nq_post_retrieval_results.json \
  --approach post-retrieval
```

Flags:

- `--config`: path to a JSON config (default: `config.json` if present).
- `--dataset`: logical dataset name (e.g. `natural_questions`, `msmarco`, `asqa`, `hagrid`).
- `--input`: input JSON with questions + documents.
- `--output`: where to save results.
- `--approach`: one of `post-retrieval`, `post-generation-llm`, `post-generation-tfidf`.

The output is a JSON list where each item contains question, generated answer, citations, and metadata.

---

## 4. Simplified pipeline runners

There are two higher-level drivers that orchestrate multiple datasets and approaches using the simplified prompt manager.

### 4.1 `run_pipeline_with_autoais_simplified.py` (answers + evaluation)

Runs the improved pipeline **and** AutoAIS evaluation in one go.

Typical usage (from Python or as a script):

```bash
python run_pipeline_with_autoais_simplified.py
```

or, inside Python:

```python
from run_pipeline_with_autoais_simplified import run_simplified_pipeline_with_autoais

run_simplified_pipeline_with_autoais(
    datasets=["natural_questions"],
    approaches=["post-retrieval", "post-generation-llm-short"],
    num_few_shot=1,
    num_examples=50,
    model_name="gpt-3.5-turbo",
    seed=42,
    save_initial_answers=True,
)
```

It will:

- Load dataset files defined in the `DATASETS` dict.
- Run the attribution pipeline for each dataset/approach.
- Save answers and (if enabled) initial answers to timestamped directories under `outputs/`.
- Run AutoAIS evaluation for each run and save metrics.

### 4.2 `run_pipeline_with_autoais_simplified_v2.py` (answers only)

Generates answers using the simplified pipeline but **does not** run evaluation. It is useful when you want to:

- Generate a batch of results first.
- Evaluate them later in a separate pass (see `evaluation_autoais_v2.py` below).

Example:

```bash
python run_pipeline_with_autoais_simplified_v2.py
```

or call `run_simplified_pipeline_answers_only(...)` from Python in a similar way to v1.

All result files are saved as `{dataset}_{approach}_results.json` in a timestamped directory.

---

## 5. AutoAIS evaluation

### 5.1 `evaluation_autoais.py` (evaluation engine)

This module implements the **AutoAIS evaluation logic**:

- Calls your Gradio Space (e.g. `user/true-model`) via `gradio_client`.
- Computes citation metrics and text metrics (precision/recall/F1, NLTK, ROUGE, BERTScore when available).
- Includes smart truncation routines for long texts.

It is imported and used by the simplified runners and by `evaluation_autoais_v2.py`.

### 5.2 `evaluation_autoais_v2.py` (evaluate saved results)

This is a **batch evaluator** for the results generated by `run_pipeline_with_autoais_simplified_v2.py`.

Usage:

```bash
python evaluation_autoais_v2.py \
  --results_dir outputs/20250101_123000_gpt_3_5_turbo_few_shot_1 \
  --batch_size 5 \
  --delay_seconds 2.0 \
  --entailment_delay 1.0
```

What it does:

- Scans `results_dir` for `*_results.json`.
- Infers `dataset` and `approach` from filenames.
- Runs `AutoAISCitationEvaluator` from `evaluation_autoais.py` for each pair.
- Aggregates metrics and writes comparison summaries.


## 6. Typical workflows

### Workflow A: Full experiment with simplified runner + AutoAIS

1. Ensure `.env` has `HF_TOKEN` and any model keys.
2. Activate venv in WSL.
3. Run:

   ```bash
   python run_pipeline_with_autoais_simplified.py
   ```

### Workflow B: Generate answers first, evaluate later

1. Generate answers only:

   ```bash
   python run_pipeline_with_autoais_simplified_v2.py
   ```

2. Evaluate all results in a directory:

   ```bash
   python evaluation_autoais_v2.py --results_dir outputs/<your_results_dir>
   ```
